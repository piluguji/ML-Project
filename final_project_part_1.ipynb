{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V_MF3_VWHLjz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-_BG9YbrHkgr"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(dataset):\n",
        "  X = dataset.iloc[:, :-1]  # Features\n",
        "  y = dataset.iloc[:, -1]   # Labels\n",
        "\n",
        "  # Separate numerical and categorical features\n",
        "  categorical_features = X.select_dtypes(include=['object']).columns\n",
        "  numerical_features = X.drop(categorical_features, axis=1)\n",
        "  categorical_features = X[categorical_features]\n",
        "\n",
        "  # Split data into train and test sets with 20-80 split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "  # Encode categorical features using LabelEncoder\n",
        "  label_encoder = LabelEncoder()\n",
        "  for column in categorical_features.columns:\n",
        "      X_train[column] = label_encoder.fit_transform(X_train[column])\n",
        "      X_test[column] = label_encoder.transform(X_test[column])\n",
        "\n",
        "\n",
        "  # Normalize data using StandardScaler for continuous features\n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AngnLj-_HOoK"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lAGhsczfMAJ0"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(X_train, X_test, y_train, y_test):\n",
        "  clf = LogisticRegression(penalty='l2', random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNhzhoahl0m"
      },
      "source": [
        "K Nearest Number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7aEG06ythk9b"
      },
      "outputs": [],
      "source": [
        "def k_nearest_neighbor(X_train, X_test, y_train, y_test):\n",
        "  clf = KNeighborsClassifier()\n",
        "  clf.fit(X_train, y_train)\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT474OkRTl0m"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nsrK6RfLTnjP"
      },
      "outputs": [],
      "source": [
        "def decision_tree(X_train, X_test, y_train, y_test):\n",
        "  clf = DecisionTreeClassifier(random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2g9Gp1CTwMV"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IfSs0jAbTqEd"
      },
      "outputs": [],
      "source": [
        "def support_vector_machine(X_train, X_test, y_train, y_test):\n",
        "  clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUlHRbwcV_LQ"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uWV_uA-2V-rw"
      },
      "outputs": [],
      "source": [
        "def random_forest(X_train, X_test, y_train, y_test, n_estimators=100, max_depth=None):\n",
        "  clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkawl7l3WCMO"
      },
      "source": [
        "Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6OcPqMbEWB7E"
      },
      "outputs": [],
      "source": [
        "def boosting(X_train, X_test, y_train, y_test, n_estimators=50, learning_rate=1.0):\n",
        "  # AdaBoost with Decision Tree as base estimator\n",
        "  base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "  clf = AdaBoostClassifier(estimator=base_estimator, n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3S0PEgtXyZ-"
      },
      "source": [
        "Cross Validation + Testing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCjTZZsyXwos",
        "outputId": "5aebd8a1-9905-4a6f-db9f-d7f38172ca84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression\n",
            "Dataset 1\n",
            "Training Accuracy (10-fold Cross-Validation): 0.9758454106280194\n",
            "\n",
            "Test Accuracy: 0.9824561403508771\n",
            "Test Precision: 1.0\n",
            "Test Recall: 0.9487179487179487\n",
            "Test F1 Score: 0.9736842105263158\n",
            "Test AUC: 0.9743589743589743\n",
            "Dataset 2\n",
            "Training Accuracy (10-fold Cross-Validation): 0.7363363363363364\n",
            "\n",
            "Test Accuracy: 0.7096774193548387\n",
            "Test Precision: 0.7037037037037037\n",
            "Test Recall: 0.5\n",
            "Test F1 Score: 0.5846153846153846\n",
            "Test AUC: 0.6772727272727272\n",
            "\n",
            "K Nearest Neighbor\n",
            "Dataset 1\n",
            "Training Accuracy (10-fold Cross-Validation): 0.9756521739130435\n",
            "\n",
            "Test Accuracy: 0.9385964912280702\n",
            "Test Precision: 0.9444444444444444\n",
            "Test Recall: 0.8717948717948718\n",
            "Test F1 Score: 0.9066666666666667\n",
            "Test AUC: 0.9225641025641026\n",
            "Dataset 2\n",
            "Training Accuracy (10-fold Cross-Validation): 0.673948948948949\n",
            "\n",
            "Test Accuracy: 0.5913978494623656\n",
            "Test Precision: 0.5\n",
            "Test Recall: 0.3157894736842105\n",
            "Test F1 Score: 0.3870967741935484\n",
            "Test AUC: 0.5488038277511963\n",
            "\n",
            "Decision Tree\n",
            "Dataset 1\n",
            "Training Accuracy (10-fold Cross-Validation): 0.9538647342995169\n",
            "\n",
            "Test Accuracy: 0.8947368421052632\n",
            "Test Precision: 0.8857142857142857\n",
            "Test Recall: 0.7948717948717948\n",
            "Test F1 Score: 0.8378378378378378\n",
            "Test AUC: 0.8707692307692306\n",
            "Dataset 2\n",
            "Training Accuracy (10-fold Cross-Validation): 0.6254504504504506\n",
            "\n",
            "Test Accuracy: 0.6344086021505376\n",
            "Test Precision: 0.5625\n",
            "Test Recall: 0.47368421052631576\n",
            "Test F1 Score: 0.5142857142857142\n",
            "Test AUC: 0.6095693779904306\n",
            "\n",
            "Support Vector Machine\n",
            "Dataset 1\n",
            "Training Accuracy (10-fold Cross-Validation): 0.9735748792270531\n",
            "\n",
            "Test Accuracy: 0.9649122807017544\n",
            "Test Precision: 0.972972972972973\n",
            "Test Recall: 0.9230769230769231\n",
            "Test F1 Score: 0.9473684210526315\n",
            "Test AUC: 0.954871794871795\n",
            "Dataset 2\n",
            "Training Accuracy (10-fold Cross-Validation): 0.7228978978978979\n",
            "\n",
            "Test Accuracy: 0.6989247311827957\n",
            "Test Precision: 0.7083333333333334\n",
            "Test Recall: 0.4473684210526316\n",
            "Test F1 Score: 0.5483870967741936\n",
            "Test AUC: 0.6600478468899522\n",
            "\n",
            "Random Forest\n",
            "Dataset 1\n",
            "Training Accuracy (10-fold Cross-Validation): 0.9603381642512077\n",
            "\n",
            "Test Accuracy: 0.9473684210526315\n",
            "Test Precision: 0.9230769230769231\n",
            "Test Recall: 0.9230769230769231\n",
            "Test F1 Score: 0.9230769230769231\n",
            "Test AUC: 0.9415384615384614\n",
            "Dataset 2\n",
            "Training Accuracy (10-fold Cross-Validation): 0.6795795795795796\n",
            "\n",
            "Test Accuracy: 0.6774193548387096\n",
            "Test Precision: 0.6818181818181818\n",
            "Test Recall: 0.39473684210526316\n",
            "Test F1 Score: 0.5\n",
            "Test AUC: 0.633732057416268\n",
            "\n",
            "Boosting\n",
            "Dataset 1\n",
            "Training Accuracy (10-fold Cross-Validation): 0.9713526570048309\n",
            "\n",
            "Test Accuracy: 0.9473684210526315\n",
            "Test Precision: 0.8837209302325582\n",
            "Test Recall: 0.9743589743589743\n",
            "Test F1 Score: 0.9268292682926831\n",
            "Test AUC: 0.9538461538461539\n",
            "Dataset 2\n",
            "Training Accuracy (10-fold Cross-Validation): 0.6630630630630632\n",
            "\n",
            "Test Accuracy: 0.6236559139784946\n",
            "Test Precision: 0.5483870967741935\n",
            "Test Recall: 0.4473684210526316\n",
            "Test F1 Score: 0.49275362318840576\n",
            "Test AUC: 0.5964114832535885\n"
          ]
        }
      ],
      "source": [
        "def evaluate_classifier(clf, X_train, X_test, y_train, y_test, cv=10):\n",
        "    # Perform 10-fold cross-validation\n",
        "    cv_results = cross_validate(clf, X_train, y_train, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'], return_estimator=True)\n",
        "    \n",
        "    accuracy = cv_results['test_accuracy'].mean()\n",
        "    precision = cv_results['test_precision'].mean()\n",
        "    recall = cv_results['test_recall'].mean()\n",
        "    f1 = cv_results['test_f1'].mean()\n",
        "    auc = cv_results['test_roc_auc'].mean()\n",
        "\n",
        "    print(f\"Training Accuracy: {accuracy}\")\n",
        "    print(f\"Training Precision: {precision}\")\n",
        "    print(f\"Training Recall: {recall}\")\n",
        "    print(f\"Training F1 Score: {f1}\")\n",
        "    print(f\"Training AUC: {auc}\")\n",
        "\n",
        "    # Find the model with the best accuracy on the validation set\n",
        "    best_model_index = max(range(10), key=lambda i: cv_results['test_accuracy'][i])\n",
        "    best_model = cv_results['estimator'][best_model_index]\n",
        "\n",
        "    # Predict on the test set using the best model\n",
        "    y_pred_test = best_model.predict(X_test)\n",
        "\n",
        "    # Print metrics on the test set\n",
        "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy_test}\")\n",
        "\n",
        "\n",
        "dataset1 = pd.read_csv('project3_dataset1.txt', delimiter='\\t')\n",
        "dataset2 = pd.read_csv('project3_dataset2.txt', delimiter='\\t')\n",
        "X_train1, X_test1, y_train1, y_test1 = preprocess_data(dataset1)\n",
        "X_train2, X_test2, y_train2, y_test2 = preprocess_data(dataset2)\n",
        "\n",
        "print(\"Logistic Regression\")\n",
        "print(\"Dataset 1\")\n",
        "clf_lr1 = logistic_regression(X_train1, X_test1, y_train1, y_test1)\n",
        "evaluate_classifier(clf_lr1, X_train1, X_test1, y_train1, y_test1)\n",
        "print(\"\\nDataset 2\")\n",
        "clf_lr2 = logistic_regression(X_train2, X_test2, y_train2, y_test2)\n",
        "evaluate_classifier(clf_lr2, X_train2, X_test2, y_train2, y_test2)\n",
        "\n",
        "print(\"\\nK Nearest Neighbor\")\n",
        "print(\"Dataset 1\")\n",
        "clf_knn1 = k_nearest_neighbor(X_train1, X_test1, y_train1, y_test1)\n",
        "evaluate_classifier(clf_knn1, X_train1, X_test1, y_train1, y_test1)\n",
        "print(\"\\nDataset 2\")\n",
        "clf_knn2 = k_nearest_neighbor(X_train2, X_test2, y_train2, y_test2)\n",
        "evaluate_classifier(clf_knn2, X_train2, X_test2, y_train2, y_test2)\n",
        "\n",
        "print(\"\\nDecision Tree\")\n",
        "print(\"Dataset 1\")\n",
        "clf_dt1 = decision_tree(X_train1, X_test1, y_train1, y_test1)\n",
        "evaluate_classifier(clf_dt1, X_train1, X_test1, y_train1, y_test1)\n",
        "print(\"\\nDataset 2\")\n",
        "clf_dt2 = decision_tree(X_train2, X_test2, y_train2, y_test2)\n",
        "evaluate_classifier(clf_dt2, X_train2, X_test2, y_train2, y_test2)\n",
        "\n",
        "print(\"\\nSupport Vector Machine\")\n",
        "print(\"Dataset 1\")\n",
        "clf_svm1 = support_vector_machine(X_train1, X_test1, y_train1, y_test1)\n",
        "evaluate_classifier(clf_svm1, X_train1, X_test1, y_train1, y_test1)\n",
        "print(\"\\nDataset 2\")\n",
        "clf_svm2 = support_vector_machine(X_train2, X_test2, y_train2, y_test2)\n",
        "evaluate_classifier(clf_svm2, X_train2, X_test2, y_train2, y_test2)\n",
        "\n",
        "print(\"\\nRandom Forest\")\n",
        "print(\"Dataset 1\")\n",
        "clf_rf1 = random_forest(X_train1, X_test1, y_train1, y_test1)\n",
        "evaluate_classifier(clf_rf1, X_train1, X_test1, y_train1, y_test1)\n",
        "print(\"\\nDataset 2\")\n",
        "clf_rf2 = random_forest(X_train2, X_test2, y_train2, y_test2)\n",
        "evaluate_classifier(clf_rf2, X_train2, X_test2, y_train2, y_test2)\n",
        "\n",
        "print(\"\\nBoosting\")\n",
        "print(\"Dataset 1\")\n",
        "clf_boosting1 = boosting(X_train1, X_test1, y_train1, y_test1)\n",
        "evaluate_classifier(clf_boosting1, X_train1, X_test1, y_train1, y_test1)\n",
        "print(\"\\nDataset 2\")\n",
        "clf_boosting2 = boosting(X_train2, X_test2, y_train2, y_test2)\n",
        "evaluate_classifier(clf_boosting2, X_train2, X_test2, y_train2, y_test2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
